# ğŸ“˜ Diabetes Prediction â€” Model Development Journey (3-Stage Progression)

A complete end-to-end evolution from baseline â†’ EDA-driven improvements â†’ advanced engineered final model.

---

## ğŸ§  Project Overview

This repository documents my complete modelling workflow for predicting **diagnosed diabetes** using structured health, lifestyle, and demographic data.

Across **three iterations**, the model evolves from a simple baseline to a highly engineered, drift-corrected, competition-ready **XGBoost pipeline**.

---

## ğŸ—‚ï¸ Project Structure

- diabetespred1.ipynb   Stage 1 â€“ Baseline Model
- diabetespred2.ipynb   Stage 2 â€“ EDA + Drift Handling + Feature Pruning
- diabetespredfinal.ipynb  Stage 3 â€“ Advanced Engineered Final Model
- test and train Data Set   (csv file)
- README.md              (this file)


# ğŸ§­ Modelling Progression â€” 3 Stages

---

## ğŸŸ¦ Stage 1 â€” Baseline Pipeline (Foundation Model)

### ğŸ¯ Goal  
Build a first working model quickly and understand the data shape.

---

### ğŸ”§ Key Steps
- Loaded train/test datasets  
- Basic `.head()`, `.info()`, `.dtypes()` inspection  
- Handled missing values (median/mode)  
- Label-encoded categoricals  
- Scaled numerical features  
- Train/validation split  
- Trained **XGBoostClassifier** with simple hyperparameters  
- Generated submission file  

---

### ğŸ’¡ What This Version Demonstrated
- Ability to set up a full ML pipeline  
- Understanding of preprocessing basics  
- First reliable benchmark for improvement  

---

### ğŸ§© Example Code Snippet â€“ Stage 1

```python
from xgboost import XGBClassifier

model = XGBClassifier(
    n_estimators=600,
    max_depth=5,
    learning_rate=0.03,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

model.fit(X_train, y_train)
valid_pred = model.predict(X_valid)
```
---


## ğŸ“Š Exploratory Feature Distributions (Stage 1 Analysis)

Before modelling, I explored how key numerical features differ between **diabetes (1)** and **non-diabetes (0)** groups.  
These histograms helped identify *high-signal* predictors and motivated feature engineering in later stages.

### ** Distribution 1 **
![Age Distribution](distribution1.png)

### ** Distribution 2 **
![BMI Distribution](distribution2.png)

### ** Distribution 3 **
![Systolic BP Distribution](distribution3.png)


---

## ğŸ§  **Key Observations from Feature Distributions**

| Feature                | Observation                 | Effect                                |
|------------------------|------------------------------|----------------------------------------|
| **Triglycerides**      | Much higher for diabetics    | Strong positive predictor              |
| **Waist-to-hip ratio** | Higher among diabetics       | Indicates central obesity risk         |
| **Systolic BP**        | Elevated in diabetics        | Hypertensionâ€“diabetes correlation      |
| **BMI**                | Higher for diabetics         | Obesity strongly linked to diabetes    |
| **Age**                | Diabetics tend to be older   | Age is a strong risk factor            |
| **Physical activity**  | Lower among diabetics        | Sedentary lifestyle increases risk     |

---

### ğŸ’¡ How This Informed Stage 2
These visual patterns revealed:

- Clear **distribution shifts** between classes  
- Strong predictors like triglycerides, BMI, BP, and WHR  
- Need to address **skew**, **drift**, and **non-linear relationships**  
- Motivation for later **feature engineering** (sleep, activity flags, log-transforms)  

This EDA stage helped shape all improvements in Stages 2 and 3.


---
## ğŸŸ© Stage 2 â€” Exploratory Data Analysis, Drift Detection & Feature Pruning

### ğŸ¯ Goal  
Understand dataset drift, unstable variables, and remove high-noise columns.

---

### ğŸ”§ Key Improvements

- âœ”ï¸ Visualized distribution drift between train/test  
- âœ”ï¸ Found noisy variables (e.g., high-cardinality categoricals)  
- âœ”ï¸ Dropped unstable or redundant features:

```python
['employment_status', 'smoking_status', 'income_level',
 'ethnicity', 'education_level']
```
---

âœ”ï¸ Fixed class imbalance:
```python
neg, pos = (y == 0).sum(), (y == 1).sum()
scale_pos_weight = neg / pos
```

âœ”ï¸ Aligned train/test columns perfectly:
```python
train, test = train.align(test, join="left", axis=1, fill_value=0)
```

---

### ğŸ’¡ What This Version Demonstrated

- Deeper understanding of feature behavior
- Responsible pruning of drift-prone inputs
- Clinical awareness: sensitivity to positive diagnosis cases
- Stronger data hygiene practices

---

### ğŸ§© Example Snippet â€“ Drift-Based Feature Removal
```python
train.drop(['employment_status','smoking_status',
            'income_level','ethnicity','education_level'], axis=1, inplace=True)

test.drop(['employment_status','smoking_status',
           'income_level','ethnicity','education_level'], axis=1, inplace=True)
```

## ğŸŸ¥ Stage 3 â€” Final Advanced Model (Feature Engineering, Drift Corrections, OHE, Robust XGBoost)

### ğŸ¯ Goal  
Build a competition-quality model with engineered features & drift-controlled transformations.

---

## ğŸ”¥ Major Upgrades in Final Model

---

### 1ï¸âƒ£ Drift Correction via Log-Transformations

```python
train[col + "_log"] = np.log1p(train[col])
test[col + "_log"]  = np.log1p(test[col])
```

- âœ”ï¸ Stabilizes skew
- âœ”ï¸ Reduces trainâ€“test distribution gaps
- âœ”ï¸ Improves generalization for tree-based models

---
### 2ï¸âƒ£ Rich Feature Engineering

- Added clinically meaningful binary flags such as:
- Physical activity anomalies
- Sleep-hour thresholds
- Screen-time indicators
- Alcohol consumption bands
- Diet pattern encodings

These capture non-linear health signals that are highly predictive for metabolic disorders.
---
### 3ï¸âƒ£ Safe One-Hot Encoding + Perfect Alignment
```python
cat_cols = train.select_dtypes(include="object").columns
train = pd.get_dummies(train, columns=cat_cols)
test  = pd.get_dummies(test, columns=cat_cols)

train, test = train.align(test, join="left", axis=1, fill_value=0)
```

- âœ”ï¸ Prevents categorical mismatch
- âœ”ï¸ Ensures reproducible inference
- âœ”ï¸ Avoids test-set leakage
---
### 4ï¸âƒ£ Dropping Entire Noisy Feature Families (Prefix Strategy)
```python
DROP_PREFIXES = ["income_", "employment_", "ethnicity_", "education_"]
DROP_COLS = [c for c in train.columns if any(c.startswith(p) for p in DROP_PREFIXES)]

train = train.drop(columns=DROP_COLS)
test  = test.drop(columns=DROP_COLS)
```

- âœ”ï¸ Removes high-variance sparse dummy columns
- âœ”ï¸ Prevents overfitting
- âœ”ï¸ Improves model stability
---
### 5ï¸âƒ£ Robust Validation with ROC-AUC
```python
roc_auc_score(y_valid, valid_proba)
```

ROC-AUC is a superior metric to accuracy for medical classification, where:
- positive cases are rare
- misclassification costs are high
---
### 6ï¸âƒ£ Final Optimized XGBoost Configuration

```python
final_model = XGBClassifier(
    n_estimators=900,
    max_depth=6,
    learning_rate=0.03,
    subsample=0.85,
    colsample_bytree=0.85,
    reg_lambda=2,
    gamma=0,
    scale_pos_weight=neg/pos,
    random_state=42,
    n_jobs=-1
)
```
- âœ”ï¸ Tuned for imbalance
- âœ”ï¸ Regularized to prevent overfitting
- âœ”ï¸ Designed for strong generalization
- âœ”ï¸ Trained on final engineered dataset

---

## ğŸ“¤ Final Submission (Stage 3)

After training the final engineered XGBoost model on the full dataset, I generated the **probability-based submission** (used for ROC-AUC evaluation).

### âœ… Saved: `submission.csv`  
This file contains:

- `id`
- `diagnosed_diabetes` (predicted probability)

### ğŸ“„ Submission Preview

| id      | diagnosed_diabetes |
|---------|---------------------|
| 700000  | 0.404538            |
| 700001  | 0.551726            |
| 700002  | 0.636839            |
| 700003  | 0.343274            |
| 700004  | 0.833294            |

*(Full table available in the notebook output.)*

---

### ğŸ§ª Sanity Check (Probability Range)

```text
Min Probability: 0.034617260098457336  
Max Probability: 0.9824862480163574
```


---


- My final model predicts with probabilities from 0.03 â†’ 0.98, which shows:

- âœ”ï¸ Confident high-risk and low-risk predictions
- âœ”ï¸ Strong ROC-AUC separation
- âœ”ï¸ Correct drift handling + engineered health signals
